# Grader Assistant Configuration

[meta]
defaultModel = "anthropic/claude-3.5-sonnet"
temperature = 0.7
purpose = "Interactive assistant for creating evaluation graders"

[contexts]
# Session context
session_id = { type = "string", question = "What is the current session ID?", description = "Unique identifier for this grader creation session" }
session_path = { type = "string", question = "What is the session folder path?", description = "Path to the current session's working directory" }
current_step = { type = "string", question = "What workflow step are we on?", description = "Current step in the grader creation workflow", default = "initial" }

# User context
user_goal = { type = "string", question = "What does the user want to evaluate?", description = "The type of AI output being evaluated (e.g., customer support, code generation)" }
evaluation_domain = { type = "string", question = "What domain or industry is this for?", description = "Context about the specific use case" }

# Grader context
grader_name = { type = "string", question = "What should we call this grader?", description = "Human-friendly name for the grader being created" }
scoring_criteria = { type = "object", question = "What are the evaluation criteria?", description = "Detailed scoring guidelines and criteria" }
sample_count = { type = "number", question = "How many samples have been gathered?", description = "Number of evaluation samples collected", default = 0 }

# Conversation context
conversation_history = { type = "object", question = "What is the recent conversation context?", description = "Recent messages for context continuity" }
last_command_result = { type = "string", question = "What was the result of the last command?", description = "Output from the most recent tool execution" }

[[tools]]
name = "create_file"
description = "Create a new file in the session directory"

[tools.parameters]
type = "object"
required = ["filename", "content"]

[tools.parameters.properties.filename]
type = "string"
description = "Name of the file to create (relative to session directory)"

[tools.parameters.properties.content]
type = "string"
description = "Content to write to the file"

[[tools]]
name = "read_file"
description = "Read a file from the session directory"

[tools.parameters]
type = "object"
required = ["filename"]

[tools.parameters.properties.filename]
type = "string"
description = "Name of the file to read (relative to session directory)"

[[tools]]
name = "list_files"
description = "List files in the session directory"

[tools.parameters]
type = "object"

[tools.parameters.properties.pattern]
type = "string"
description = "Optional glob pattern to filter files"

[[tools]]
name = "run_gather"
description = "Run aibff gather command to collect evaluation samples"

[tools.parameters]
type = "object"
required = ["input_file"]

[tools.parameters.properties.input_file]
type = "string"
description = "Path to the file containing samples"

[tools.parameters.properties.output_file]
type = "string"
description = "Output file for processed samples (defaults to samples.jsonl)"

[[tools]]
name = "run_eval"
description = "Run aibff eval command to test the grader"

[tools.parameters]
type = "object"
required = ["grader_file"]

[tools.parameters.properties.grader_file]
type = "string"
description = "Path to the grader deck file"

[tools.parameters.properties.samples_file]
type = "string"
description = "Path to samples file (uses embedded samples if not specified)"

[tools.parameters.properties.output_folder]
type = "string"
description = "Output folder for results (defaults to 'results')"

[[tools]]
name = "update_progress"
description = "Update the session progress tracking"

[tools.parameters]
type = "object"
required = ["step", "status"]

[tools.parameters.properties.step]
type = "string"
description = "Current workflow step name"

[tools.parameters.properties.status]
type = "string"
description = "Status message or progress update"

[tools.parameters.properties.metadata]
type = "object"
description = "Additional metadata to store"

# Sample conversations demonstrating good patterns
[[samples]]
id = "welcome_interaction"
userMessage = "I want to create a grader for customer support responses"
assistantResponse = """Welcome! I'm excited to help you create a grader for customer support responses. This is a great use case for systematic evaluation.

To get started, I'd like to understand your specific needs better:
1. What kind of customer support are we talking about? (e.g., technical support, billing inquiries, general questions)
2. Do you already have some example responses we can work with?
3. What aspects of the responses are most important to you? (e.g., accuracy, helpfulness, tone, resolution speed)

This will help me create a grader that matches your exact requirements."""
score = 3
description = "Welcoming response that gathers initial requirements"

[[samples]]
id = "sample_guidance"
userMessage = "I have about 50 customer emails and our support team's responses. How should I format them?"
assistantResponse = """Great! Having real examples will make our grader much more effective. For the aibff system, we'll need to format your samples in a specific way.

Each sample should have:
- `userMessage`: The customer's original email/question
- `assistantResponse`: Your support team's response
- `score` (optional): If you've already rated some as good/bad examples

Here's the format we need (JSONL - one JSON object per line):
```json
{"userMessage": "I can't log into my account...", "assistantResponse": "I'm sorry to hear that..."}
{"userMessage": "How do I cancel my subscription?", "assistantResponse": "To cancel your subscription..."}
```

Would you like me to help you convert your emails into this format? I can guide you through it, or if you share a few examples, I can show you exactly how to structure them."""
score = 3
description = "Clear formatting guidance with examples"

[[samples]]
id = "criteria_definition"
userMessage = "What makes a good vs bad customer support response?"
assistantResponse = """Excellent question! Let's define clear criteria for your grader. Based on customer support best practices, here's what I suggest we evaluate:

**Positive indicators (+1 to +3):**
- ✓ Directly addresses the customer's issue
- ✓ Provides clear, actionable solutions
- ✓ Shows empathy and understanding
- ✓ Professional and friendly tone
- ✓ Complete resolution or clear next steps

**Negative indicators (-1 to -3):**
- ✗ Ignores the main problem
- ✗ Gives incorrect information
- ✗ Dismissive or robotic tone
- ✗ Overly technical without explanation
- ✗ No clear resolution path

We'll use a scale from -3 (terrible) to +3 (excellent), with 0 being neutral/acceptable.

Would you like to refine these based on your specific business needs? For example, do you have specific policies or tone guidelines we should incorporate?"""
score = 3
description = "Comprehensive criteria explanation with scoring scale"

[[samples]]
id = "iteration_feedback"
userMessage = "The grader seems too harsh. Most responses are getting negative scores."
assistantResponse = """I see the issue. Let's analyze why the grader might be too strict and adjust it together.

Looking at the results, I notice the grader is penalizing responses heavily for minor issues. This often happens when criteria are too rigid. Let me suggest some adjustments:

1. **Adjust the baseline**: Instead of starting from "perfect," let's assume responses are generally good (0) and adjust from there
2. **Weight criteria differently**: Perhaps tone is less critical than problem resolution for your use case?
3. **Add positive examples**: Including more examples of good responses in the grader helps calibrate expectations

Would you like to review a few specific examples where you disagree with the scores? That will help me fine-tune the criteria to match your standards better."""
score = 3
description = "Helpful iteration guidance addressing scoring calibration"